<html>
	<head>
		<link rel="stylesheet" type="text/css" href="css/main.css">
	</head>
    
    <nav>
      <ul id="navbar"> <h3>
        <li><a href="https://kathleenisrad.github.io">Home</a></li>
	      </h3>
      </ul>
    </nav>
 <body>

<br>
	 <br>
<br>
	 <br>
<br>
<h2><center> Glomeruli Detection and Segmentation <center> </h2>
	 <p>The goal of this project is to identify glomeruli in images of kidney slices.
I used both Faster R-CNN and Mask R-CNN and compare their performances below.  </p>

<br>
<br>

<br>
	 <br>

<h3 id = "faster"> <center> Faster R-CNN Results:</center>  </h3>
<h3> <center> <a href="https://github.com/kathleenisrad/glomeruli-detection-and-segmentation/blob/main/code/02-faster-rcnn-pytorch.ipynb"> Faster R-CNN notebook </a> </center> </h3>
	 <p> Sample image from the training images, that was split into training and testing sets.
	 Black boxes are ground truth, white boxes are my model's predictions:  </p>
	 <p><center> <img src = "https://i.imgur.com/XDK1nJA.jpg"></center> </p>
<br>
	 <p>Sample image from the testing images (no ground truth):  </p>
<p><center> <img src = "https://i.imgur.com/kD1zesY.jpg"></center>
</p>
<br>
<br>
<br>
	 <br>


<h3 id = "mask"> <center> Mask R-CNN Results: </center>  </h3>
<h3> <center> <a href="https://github.com/kathleenisrad/glomeruli-detection-and-segmentation/blob/main/code/05-mask-rcnn-pytorch.ipynb"> Mask R-CNN notebook </a> </center> </h3>  

<p>Sample image from the training images, that was split into training and testing sets. 
Black box is ground truth, grey box is my model's prediction, mask is the aurora colored part:  </p>

<p><img src = "https://i.imgur.com/XJiorfC.jpg">  </p>

<p>Sample image from the testing images (no ground truth or bounding boxes):</p>
<p><img src = "https://i.imgur.com/rhIAv7R.jpg">  </p>
<br>
<br>
<br>
<br>
<h3 id = "data"> <center>  Dataset:  </center>  </h3>
<p>8 training images and 5 testing images were provided. The original images were too large to push onto github. They can be downloaded from here:  </p>
<p><a href = "https://www.kaggle.com/c/hubmap-kidney-segmentation/data"> Kaggle Dataset </a></p>

<br>
<br>
	<br>
<br>

<h3 id = "file"> <center>  File Structure:  </center>  </h3>
<p>My folders are organized a little differently than the way the original zip file is organized. You will need to move the training and test images to the locations listed below in order for the code to work.
My code will eventually add additional folders called masks, grey and slices.  </p>

<br>

<div style="text-align: center;">
    <div style="display: inline-block; text-align: left;">
<pre>[me@home]$ tree main  
<b>main</b>
├── code  
├── CSVs
|   └── <b>ORIGINAL CSVs GO HERE</b> 
├── test  
│   ├── images  
│   │   └── <b>ORIGINAL TEST IMAGES GO HERE</b>  
│   │
│   └── JSONs      
│       └── <b>ORIGINAL TEST JSON FILES GO HERE  </b> 
│       
└── train  
    ├── images  
    │   └── <b>ORIGINAL TRAIN IMAGES GO HERE</b>   
    │    
    └── JSONs     
        └── <b>ORIGINAL TRAIN JSON FILES GO HERE</b>  </pre>
    </div>
</div>
	
	
	

<br>
<br>
<br>
	 <br>

<h3 id="eda"> <center>  EDA: </center>  </h3>
<h3> <center>  <a href="https://github.com/kathleenisrad/glomeruli-detection-and-segmentation/blob/main/code/00-kidney-images-EDA.ipynb"> EDA notebook </a>  </center>  </h3>
<p>I performed some exploratory data analysis on the original images to familiarize myself with the dataset.
Each image was gigantic — many were over 1GB, and the largest was a whopping 4GB! — and so my computer wasn’t able to open the images using a traditional image viewer. So instead, I resized each image and plotted them: </p>

<p><img src = "https://i.imgur.com/cDRto6q.jpg">   </p>
<br>
<p>Next, I visualized the masks. The masks provided were actually RLE encoded representations, and someone was nice enough to post the function they made to turn the RLE image into a numpy array. I then took this numpy array and turned it into an image: </p>  
<br>
<p><img src = "https://i.imgur.com/41NbedZ.jpg"> </p>
<br>
<br>
<p>I overlaid these masks on the images to see my target cells: </p>

<p><img src = "https://i.imgur.com/aJyMQHq.jpg"> </p>

<br>
<br>
<p>Here is a close up of the targets: </p>
<p><img src = "https://i.imgur.com/NzkpcKK.jpg"> </p>

<br>
<br>
<br>
<br>

<h3 id = "process"> <center>  Preprocessing the images: </center>  </h3>
<h3> <center>  <a href="https://github.com/kathleenisrad/glomeruli-detection-and-segmentation/blob/main/code/01-preprocessing-kidney-images.ipynb"> Preprocessing notebook </a>  </center>  </h3>


<p>Steps I took to prepare my images for training:</p>
	<br>
 <p>I resized both training and testing to about 50%, then turned them into greyscale in an effort to make their file sizes a bit smaller. </p>
	<br>
 <p>Sliced the resulting images either 16x16 or 32x32, depending on image dimensions. </p>
	<br>
 <p>Example slice: </p>
	<center> <img src = "https://i.imgur.com/3dXIUlh.jpg"> </center>
	<br>
 <p>Resized and sliced the masks to match the training images. Then I deleted all the slices from the masks and training images that didn't have any target cells. </p>
	<br>
 <p>Turned each instance in each mask slice into a different color: </p>
     <center> <img src="https://i.imgur.com/VoDzBF5.jpg"> </center>
	<br>
 <p>Made bounding boxes around each instance and recorded the x and y coordinates for each bounding box in a CSV. </p>
<br>
	<br>
	<br>
	<br>
	<br>
     </p>
     </body>
</html>
